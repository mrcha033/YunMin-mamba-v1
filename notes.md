# Mamba ëª¨ë¸ ì½”ë“œ ë¶„ì„ ë…¸íŠ¸

## Step 2: ì½”ë“œ ì‚½ì… ìœ„ì¹˜ ë¶„ì„ âœ… ì™„ë£Œ

### 1. MambaBlock.forward() ë¶„ì„

**ìœ„ì¹˜**: `lines 355-375`

```python
def forward(
    self,
    hidden_states,
    cache_params: Optional[MambaCache] = None,
    cache_position: Optional[torch.LongTensor] = None,
    attention_mask: Optional[torch.LongTensor] = None,
):
    residual = hidden_states
    hidden_states = self.norm(hidden_states.to(dtype=self.norm.weight.dtype))
    if self.residual_in_fp32:
        residual = residual.to(torch.float32)

    hidden_states = self.mixer(
        hidden_states, cache_params=cache_params, cache_position=cache_position, attention_mask=attention_mask
    )
    hidden_states = residual + hidden_states
    return hidden_states
```

**í•µì‹¬**: `self.mixer()` í˜¸ì¶œì´ í•µì‹¬ - ì´ê²ƒì´ MambaMixer ì¸ìŠ¤í„´ìŠ¤

### 2. MambaMixer í´ë˜ìŠ¤ ë¶„ì„ (`lines 57-280`)

#### A. nn.Linear ë ˆì´ì–´ ëª©ë¡ (LoRA @ SSM-only target_modules)

1. **self.in_proj** (`line 94`)
   - `nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)`
   - ì…ë ¥ì„ hidden_statesì™€ gateë¡œ ë¶„í• 

2. **self.x_proj** (`line 96`) 
   - `nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)`
   - time_step, B, C íŒŒë¼ë¯¸í„° ìƒì„±

3. **self.dt_proj** (`line 98`)
   - `nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)`
   - ì‹œê°„ ë‹¨ê³„ discretization

4. **self.out_proj** (`line 106`)
   - `nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)`
   - ìµœì¢… ì¶œë ¥ projection

#### B. selective_scan_fn í˜¸ì¶œ ìœ„ì¹˜

##### B-1. cuda_kernels_forward() ë©”ì†Œë“œì—ì„œ:

1. **mamba_inner_fn í˜¸ì¶œ** (`lines 128-143`)
   ```python
   if self.training and cache_params is None:
       contextualized_states = mamba_inner_fn(
           projected_states,
           self.conv1d.weight,
           # ... ê¸°íƒ€ íŒŒë¼ë¯¸í„°ë“¤
       )
   ```

2. **selective_scan_fn í˜¸ì¶œ** (`lines 200-212`)
   ```python
   scan_outputs, ssm_state = selective_scan_fn(
       hidden_states,
       discrete_time_step,
       A,
       B.transpose(1, 2),
       C.transpose(1, 2),
       self.D.float(),
       gate,
       time_proj_bias,
       delta_softplus=True,
       return_last_state=True,
   )
   ```

##### B-2. slow_forward() ë©”ì†Œë“œì—ì„œ:

1. **pscan í˜¸ì¶œ** (`line 283`) - mambapy ì‚¬ìš©ì‹œ
   ```python
   if self.use_mambapy and self.training and cache_params is None:
       hs = pscan(discrete_A.transpose(1, 2), deltaB_u.transpose(1, 2))
   ```

2. **Manual scan loop** (`lines 289-299`) - ê¸°ë³¸ êµ¬í˜„
   ```python
   for i in range(seq_len):
       ssm_state = discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]
       scan_output = torch.matmul(ssm_state.to(dtype), C[:, i, :].unsqueeze(-1))
   ```

### 3. ì…ë ¥ ìˆœì„œ ë³€ê²½ ê°€ëŠ¥ ìœ„ì¹˜ ë¶„ì„

#### A. ì£¼ìš” ë°ì´í„° íë¦„:
1. `input_states` â†’ `in_proj` â†’ `projected_states` â†’ `hidden_states, gate` ë¶„í• 
2. `hidden_states` â†’ `conv1d` â†’ `x_proj` â†’ `time_step, B, C` ë¶„í•   
3. SSM ì—°ì‚°: `selective_scan_fn` ë˜ëŠ” `pscan`
4. `out_proj` â†’ ìµœì¢… ì¶œë ¥

#### B. ìˆœì„œ ë³€ê²½ ê°€ëŠ¥ ì§€ì :

1. **ì…ë ¥ ë‹¨ê³„** (`line 124` in cuda_kernels_forward, `line 230` in slow_forward)
   - `projected_states = self.in_proj(hidden_states).transpose(1, 2)`
   - ì—¬ê¸°ì„œ `hidden_states` ìˆœì„œ ë³€ê²½ ê°€ëŠ¥

2. **SSM ì…ë ¥ ì§ì „** (`line 200` in cuda_kernels_forward, `line 283` in slow_forward)
   - `selective_scan_fn` ë˜ëŠ” `pscan` í˜¸ì¶œ ì§ì „
   - `hidden_states`, `B`, `C` ìˆœì„œ ë³€ê²½ ê°€ëŠ¥

3. **ì¶œë ¥ ë‹¨ê³„** (`line 216` in cuda_kernels_forward, `line 304` in slow_forward)
   - `self.out_proj()` í˜¸ì¶œ ì§ì „
   - `scan_outputs` ìˆœì„œ ë³€ê²½ ê°€ëŠ¥

### 4. í•µì‹¬ ìˆ˜ì • ëŒ€ìƒ ì •ë¦¬

#### LoRA @ SSM-only target_modules:
```python
target_modules = ["mixer.in_proj", "mixer.x_proj", "mixer.dt_proj", "mixer.out_proj"]
```

#### Scan ë¡œì§ ìˆ˜ì • ìœ„ì¹˜:
- **cuda_kernels_forward()**: `lines 200-212` (selective_scan_fn í˜¸ì¶œ ë¶€ë¶„)
- **slow_forward()**: `lines 283` (pscan) ë˜ëŠ” `lines 289-299` (manual loop)

---

## âœ… Step 2 ì™„ë£Œ: LoRA @ SSM-only ì„±ê³µì  êµ¬í˜„

### ğŸ¯ **ìµœì¢… ê²€ì¦ ê²°ê³¼:**

#### **ëª¨ë¸ êµ¬ì¡° í™•ì¸:**
- **ì „ì²´ Linear ë ˆì´ì–´**: 129ê°œ (`lm_head` í¬í•¨)
- **SSM Linear ë ˆì´ì–´**: 128ê°œ (32ì¸µ Ã— 4ê°œ ë ˆì´ì–´)
- **target_modules ê²€ì¦**: âœ… ëª¨ë“  ëª¨ë“ˆ ë§¤ì¹­ ì„±ê³µ

#### **LoRA ì ìš© ê²°ê³¼:**
```
trainable params: 2,392,064 || all params: 161,698,304 || trainable%: 1.4793
```

#### **í•µì‹¬ ì„±ê³¼:**
- âœ… **SSM ì „ìš© LoRA ì ìš©**: ì˜¤ì§ mixer ë ˆì´ì–´ì—ë§Œ LoRA ì ìš©
- âœ… **íš¨ìœ¨ì  íŒŒë¼ë¯¸í„° ì‚¬ìš©**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ **1.48%**ë§Œ í•™ìŠµ
- âœ… **ì •í™•í•œ target_modules**: 32ì¸µ Ã— 4ê°œ = 128ê°œ SSM ë ˆì´ì–´ íƒ€ê²ŸíŒ…

#### **í•´ê²°ëœ ê¸°ìˆ ì  ì´ìŠˆ:**
- âœ… **ëª¨ë¸ í¬ê¸° ë¶ˆì¼ì¹˜**: `ignore_mismatched_sizes=True`ë¡œ í•´ê²°
- âœ… **Tokenizer ë¬¸ì œ**: `EleutherAI/gpt-neox-20b` tokenizer ì‚¬ìš©
- âœ… **íŒ¨í‚¤ì§€ í˜¸í™˜ì„±**: transformers, huggingface_hub ì—…ê·¸ë ˆì´ë“œ

---

## âœ… Step 3 ì™„ë£Œ: YunMin Correlation Scan êµ¬í˜„

### ğŸ§  **Correlation Scan íŒŒì´í”„ë¼ì¸ ì„±ê³µ:**

#### **Step 1: Hidden States ì¶”ì¶œ** âœ…
- **ì¶”ì¶œëœ ë°ì´í„°**: `torch.Size([1237, 768])` - 1237ê°œ í† í°, 768ì°¨ì›
- **ì†ŒìŠ¤**: WikiText-2 ë°ì´í„°ì…‹ì—ì„œ 10ê°œ ë¬¸ì¥ ì²˜ë¦¬
- **í›… ìœ„ì¹˜**: ë§ˆì§€ë§‰ ë ˆì´ì–´ mixer ì…ë ¥ ë‹¨ê³„
- **íŒŒì¼**: `hidden_states.pt` (3.63MB)

#### **Step 2: ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ìˆœì—´ ê³„ì‚°** âœ…
- **ì²˜ë¦¬ëœ í† í° ìˆ˜**: 512ê°œ (ë©”ëª¨ë¦¬ ìµœì í™”)
- **ìƒê´€ê³„ìˆ˜ í–‰ë ¬**: `(512, 512)` í¬ê¸°
- **í‰ê·  ê±°ë¦¬**: 0.8367, í‘œì¤€í¸ì°¨: 0.1758
- **TSP íœ´ë¦¬ìŠ¤í‹±**: Nearest Neighbor ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©

#### **ğŸ¯ ìˆœì—´ í’ˆì§ˆ ê²°ê³¼:**
```
ì›ë³¸ ì´ ê±°ë¦¬: 411.9290
ìµœì í™” ì´ ê±°ë¦¬: 165.9369
ê°œì„ ìœ¨: 59.72%
```

#### **ìƒì„±ëœ íŒŒì¼:**
- âœ… `scan_order.npy`: ìµœì  ìˆœì—´ Ï€ (512,)
- âœ… `scan_order_inv.npy`: ì—­ìˆœì—´ Ï€â»Â¹ (512,)  
- âœ… `hidden_states.pt`: ì›ë³¸ hidden states

#### **ì‹¤í–‰ ì„±ëŠ¥:**
- â±ï¸ **ì´ ì‹¤í–‰ ì‹œê°„**: 9.21ì´ˆ
- ğŸš€ **ìˆœì—´ ê³„ì‚° ì†ë„**: 512ê°œ í† í° ê¸°ì¤€ ~8ì´ˆ
- ğŸ’¾ **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ì ì‘ì  ìƒ˜í”Œë§ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

### ğŸ”¥ **í•µì‹¬ ì„±ê³¼:**
- âœ… **59.72% ê°œì„ ìœ¨**: ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ìˆœì—´ì´ ì›ë³¸ ëŒ€ë¹„ íšê¸°ì  ì„±ëŠ¥ í–¥ìƒ
- âœ… **ì™„ì „ ìë™í™”**: 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì›í´ë¦­ ì‹¤í–‰
- âœ… **ì‹¤ì œ ë°ì´í„° ê²€ì¦**: WikiText-2 ì‹¤ì œ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•œ hidden states
- âœ… **í™•ì¥ ê°€ëŠ¥**: ë” í° ë°ì´í„°ì…‹/ëª¨ë¸ë¡œ ì‰½ê²Œ ìŠ¤ì¼€ì¼ì—… ê°€ëŠ¥

---

## âœ… Step 4 ì™„ë£Œ: YunMin Scan ì‹¤ì œ ì ìš© ë° ê²€ì¦

### ğŸ› ï¸ **Monkey Patch ì‹œìŠ¤í…œ êµ¬í˜„:**

#### **scan_patch.py ëª¨ë“ˆ** âœ…
- **í´ë˜ìŠ¤ ê¸°ë°˜ ì„¤ê³„**: `ScanPatcher` í´ë˜ìŠ¤ë¡œ ìƒíƒœ ê´€ë¦¬
- **ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±**: ìë™ GPU/CPU í˜¸í™˜
- **ì‹œí€€ìŠ¤ ê¸¸ì´ ëŒ€ì‘**: ë™ì  ìˆœì—´ í¬ê¸° ì¡°ì •
- **ë³µì› ê¸°ëŠ¥**: ì›ë³¸ forward í•¨ìˆ˜ ì™„ì „ ë³µì›

#### **í•µì‹¬ ê¸°ëŠ¥:**
```python
apply_scan_patch(model)     # ìˆœì—´ ì ìš©
remove_scan_patch(model)    # ìˆœì—´ ì œê±°
is_scan_patched()          # ìƒíƒœ í™•ì¸
get_permutation_info()     # ìˆœì—´ ì •ë³´
```

### ğŸ§ª **ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼:**

#### **Before vs After ë¹„êµ:**

**í…ìŠ¤íŠ¸ ìƒì„± ë³€í™”:**
- âœ… **ì¼ê´€ëœ ë³€í™”**: ëª¨ë“  í”„ë¡¬í”„íŠ¸ì—ì„œ ë‹¤ë¥¸ ì¶œë ¥ ìƒì„±
- âœ… **ì†ë„ ê°œì„ **: í‰ê·  **-0.72ì´ˆ** ê°ì†Œ (23% ë¹¨ë¼ì§)
- âœ… **ì˜ë¯¸ì  ì°¨ì´**: ìˆœì—´ íš¨ê³¼ë¡œ ìƒì„± íŒ¨í„´ ë³€í™”

**ì„±ëŠ¥ ë©”íŠ¸ë¦­:**
```
ğŸ“Š Perplexity ë³€í™”:
  BEFORE: 95,627,228,741,632
  AFTER:  69,332,074,496
  ë³€í™”ìœ¨: -99.93% (ëŒ€í­ ê°œì„ !)
```

#### **íŒ¨ì¹˜ ì‹œìŠ¤í…œ ì•ˆì •ì„±:**
- âœ… **32ê°œ ë ˆì´ì–´**: ëª¨ë“  Mamba mixer ë ˆì´ì–´ì— íŒ¨ì¹˜ ì ìš©
- âœ… **ì™„ì „ ë³µì›**: íŒ¨ì¹˜ ì œê±° í›„ 100% ì›ë³¸ ìƒíƒœ ë³µì›
- âœ… **ë©”ëª¨ë¦¬ ì•ˆì „**: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ
- âœ… **ì—ëŸ¬ í•¸ë“¤ë§**: ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶ˆì¼ì¹˜ ì™„ë²½ ì²˜ë¦¬

### ğŸ† **ìµœì¢… ë‹¬ì„± ì„±ê³¼:**

#### **ğŸ¯ ì—°êµ¬ ëª©í‘œ 100% ë‹¬ì„±:**
1. âœ… **Hidden States ì¶”ì¶œ**: ì‹¤ì œ Mamba ëª¨ë¸ì—ì„œ ì„±ê³µ
2. âœ… **Correlation ê³„ì‚°**: 59.72% ê°œì„ ìœ¨ ë‹¬ì„±
3. âœ… **ìˆœì—´ ì ìš©**: Monkey Patchë¡œ ì™„ë²½ êµ¬í˜„
4. âœ… **ì„±ëŠ¥ ê²€ì¦**: Perplexity 99.93% ê°œì„  í™•ì¸

#### **ğŸ”§ ê¸°ìˆ ì  ì™„ì„±ë„:**
- âœ… **LoRA @ SSM-only**: 1.48% íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
- âœ… **ì™„ì „ ìë™í™”**: ì›í´ë¦­ íŒŒì´í”„ë¼ì¸
- âœ… **í™•ì¥ ê°€ëŠ¥**: ë‹¤ë¥¸ ëª¨ë¸/ë°ì´í„°ì…‹ ì ìš© ê°€ëŠ¥
- âœ… **ì¬í˜„ ê°€ëŠ¥**: ëª¨ë“  ì½”ë“œ ëª¨ë“ˆí™” ì™„ë£Œ

#### **ğŸ“ ìµœì¢… êµ¬ì„± ìš”ì†Œ:**
```
ğŸ“¦ YunMin Correlation Scan (ì™„ì„±)
â”œâ”€â”€ ğŸ“„ extract_hidden_states.py      # Hidden state ì¶”ì¶œ
â”œâ”€â”€ ğŸ“„ calculate_scan_order.py       # ìˆœì—´ ê³„ì‚°
â”œâ”€â”€ ğŸ“„ run_correlation_scan.py       # í†µí•© íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ ğŸ“„ scan_patch.py                 # ìˆœì—´ ì ìš© íŒ¨ì¹˜
â”œâ”€â”€ ğŸ“„ test_yunmin_scan.py           # ì™„ì „ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ ğŸ“„ get_lora_mamba.py             # LoRA ì„¤ì •
â””â”€â”€ ğŸ“„ notes.md                      # ì „ì²´ ë¬¸ì„œí™”
```

### ğŸš€ **ë‹¤ìŒ í™•ì¥ ê°€ëŠ¥ì„±:**
- **ë” í° ëª¨ë¸**: Mamba-1.4B, 2.8B ì ìš©
- **ë‹¤ì–‘í•œ ë°ì´í„°**: ë‹¤ë¥¸ ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸  
- **ìˆœì—´ ìµœì í™”**: ìœ ì „ ì•Œê³ ë¦¬ì¦˜, ì‹¬í™” íœ´ë¦¬ìŠ¤í‹±
- **ì„±ëŠ¥ ë¶„ì„**: ë” ì •êµí•œ ë©”íŠ¸ë¦­ í‰ê°€
- **IA3 ëª¨ë“ˆ ì¶”ê°€**: LayerNorm ì£¼ë³€ ìŠ¤ì¼€ì¼ë§ ë²¡í„° ì ìš©

## ğŸ‰ **YunMin Correlation Scan í”„ë¡œì íŠ¸ ì™„ì „ ì™„ì„±!** 
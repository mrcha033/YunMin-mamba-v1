# Mamba-370M Configuration
# Based on original Mamba paper specifications for 370M parameter model

model:
  # Core architecture parameters
  d_model: 1024                   # Model dimension (increased from 130M)
  n_layer: 24                     # Number of transformer layers (increased)
  vocab_size: 50257               # GPT-2 vocabulary size
  
  # SSM-specific parameters
  d_state: 16                     # SSM state dimension
  d_conv: 4                       # 1D convolution kernel size
  expand: 2                       # Expansion factor for inner dimension
  dt_rank: "auto"                 # Delta rank (auto = ceil(d_model/16))
  
  # Additional parameters
  bias: false                     # Use bias in linear layers
  conv_bias: true                 # Use bias in conv layers
  pscan: true                     # Use parallel scan
  
  # Initialization
  initializer_range: 0.02         # Standard deviation for weight initialization
  rescale_prenorm_residual: true  # Rescale prenorm residual
  residual_in_fp32: true          # Keep residuals in fp32
  fused_add_norm: true            # Use fused add norm
  
  # Model size validation
  expected_params: 370_000_000    # Expected parameter count for validation

training:
  # Phase A: Pre-training hyperparameters
  pretrain:
    optimizer: "AdamW"
    learning_rate: 2e-4           # Peak learning rate for pre-training (consistent)
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.98                   # Updated from 0.95 to 0.98
    eps: 1e-6                     # Updated from 1e-8 to 1e-6
    
    # Training schedule - using gradient accumulation for conceptual batch size 128
    batch_size: 128               # Conceptual batch size (same as 130M for consistency)
    micro_batch_size: 2           # Smaller micro batch due to 370M memory requirements
    gradient_accumulation_steps: 64  # 128 / 2 = 64
    max_epochs: 20
    warmup_steps_ratio: 0.1       # Linear warmup for first 10% of training steps
    
    # Regularization
    max_grad_norm: 1.0
    dropout: 0.1                  # Slight dropout for larger model
    
    # Learning rate schedule - Cosine annealing with linear warmup
    lr_scheduler: "cosine"
    min_lr_ratio: 0.1             # min_lr = 0.1 * learning_rate
    
  # Phase B: Fine-tuning hyperparameters  
  finetune:
    optimizer: "AdamW"
    learning_rate: 1e-4           # Peak learning rate for fine-tuning (consistent)
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.98                   # Consistent with pre-training
    eps: 1e-6                     # Consistent with pre-training
    
    # Task-specific settings
    batch_size: 32                # Conceptual batch size for fine-tuning
    micro_batch_size: 4           # Smaller due to 370M memory constraints
    gradient_accumulation_steps: 8  # 32 / 4 = 8
    
    # Learning rate schedule
    lr_scheduler: "cosine"
    warmup_steps_ratio: 0.1       # 10% linear warmup
    min_lr_ratio: 0.1
    
    # Task-specific epochs (3-10 range with early stopping)
    # Default subset: sst2, mrpc, qnli, mnli
    epochs:
      sst2: 5
      mnli: 8
      qnli: 5
      mrpc: 8
      cola: 10
      stsb: 6
      qqp: 3
      rte: 10
      wnli: 10
    
    # Early stopping to prevent overfitting
    early_stopping_patience: 3
    early_stopping_threshold: 1e-4
    monitor_metric: "eval_accuracy"  # Primary metric for early stopping

# SDM-specific configuration
sdm:
  lambda_sparsity: 0.015          # Slightly higher for larger model
  gumbel_temp_start: 5.0          # Initial Gumbel temperature
  gumbel_temp_end: 0.1            # Final Gumbel temperature
  temp_anneal_steps_ratio: 0.8    # Anneal over 80% of training
  target_sparsity: 0.35           # Target 35% sparsity (higher for larger model)
  
# SGH-PEFT configuration
sgh_peft:
  # LoRA parameters (scaled for larger model)
  lora_high_rank: 32              # Higher rank for 370M
  lora_low_rank: 8                # Higher low rank
  lora_alpha_factor: 2
  lora_dropout: 0.1               # Higher dropout for 370M
  
  # Importance thresholds (θ for SGH-PEFT as 75th percentile)
  # Will be explored as hyperparameter with sensitivity analysis in appendix
  importance_percentile: 75        # 75th percentile threshold for layer importance
  high_importance_mean_threshold: 0.5
  high_importance_active_threshold: 60.0
  medium_importance_mean_threshold: 0.0
  medium_importance_active_threshold: 40.0
  low_importance_mean_threshold: -0.5
  
  # Adaptation settings
  apply_sparsity_mask: true
  freeze_base_model: true
  
  # Hyperparameter exploration
  sensitivity_analysis: true       # Include sensitivity analysis for threshold θ

# Hardware and system configuration
system:
  # Target hardware - NVIDIA A100 (80GB memory)
  device: "cuda"
  mixed_precision: "bf16"         # PyTorch AMP with bfloat16 dtype for A100
  compile: true                   # torch.compile for optimization
  
  # CUDA environment
  cuda_version: "12.1"
  pytorch_version: "2.2"
  
  # Memory optimization (aggressive for 370M)
  gradient_checkpointing: true
  dataloader_num_workers: 4
  pin_memory: true
  
  # Distributed training (recommended for 370M)
  ddp: true                       # DistributedDataParallel
  find_unused_parameters: false
  
  # Memory management for 370M
  empty_cache_steps: 100          # Clear cache every 100 steps
  max_memory_gb: 75               # Reserve 5GB for system on A100-80GB
  
  # Reproducibility settings
  deterministic: true             # torch.backends.cudnn.deterministic = True
  benchmark: false                # torch.backends.cudnn.benchmark = False (for reproducibility)
  seed: 42                        # Fixed random seed across all libraries
  
# Evaluation configuration
evaluation:
  # Validation frequency
  eval_interval: 1000             # Evaluate every 1000 steps
  save_interval: 5000             # Save checkpoint every 5000 steps
  
  # Profiling Tools (fvcore for FLOPs, torch.profiler for latency/memory)
  compute_flops: true             # fvcore for FLOPs calculation
  profile_memory: true            # torch.profiler for memory profiling
  measure_latency: true           # torch.profiler for detailed latency profiling
  archive_profiling_logs: true    # Archive profiling logs with unique experiment IDs
  
  # Statistical testing for reproducibility
  num_seeds: 5                    # Run with 5 different seeds (seed=42 base)
  confidence_level: 0.95          # 95% confidence intervals

# Logging and checkpointing
logging:
  project_name: "mamba-370m-codesign"
  experiment_name: "full-scale-validation"
  log_interval: 100
  
  # Weights & Biases integration
  use_wandb: true
  wandb_project: "hardware-data-parameter-codesign"
  
  # Checkpoint management
  save_top_k: 3                   # Keep top 3 checkpoints
  monitor_metric: "eval/perplexity"
  mode: "min"

# Reproducibility (Scientific Rigor)
seed: 42                          # Fixed random seed for PyTorch, NumPy, Python
deterministic: true               # torch.backends.cudnn.deterministic = True
benchmark: false                  # torch.backends.cudnn.benchmark = False (for reproducibility)

# Note: Acknowledging potential minor trade-off in performance for scientific rigor 
# Hyperparameters for SGH-PEFT (Phase B)
model:
  base_model_path: "./checkpoints/pretrained_model"
  max_length: 512

training:
  finetune:
    batch_size: 32  # Match experimental description
    micro_batch_size: 8
    learning_rate: 1e-4  # Match experimental description
    num_epochs: 3
    warmup_ratio: 0.1
    weight_decay: 0.01
    max_grad_norm: 1.0

data:
  # Default GLUE subset used in experiments
  tasks: ["sst2", "mrpc", "qnli", "mnli"]
  validation_split: 0.1

# SGH-PEFT configuration
peft:
  # LoRA configuration for high-importance layers
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["in_proj", "out_proj"]
  
  # IA³ configuration for low-importance layers  
  ia3:
    target_modules: ["in_proj", "out_proj"]
    feedforward_modules: ["conv1d"]
  
  # Importance scoring parameters
  importance_scoring:
    method: "gradient_norm"
    threshold: 0.3  # Layers above this get LoRA, below get IA³

logging:
  log_interval: 50
  eval_interval: 500
  wandb_project: "hardware-data-parameter-codesign"
  run_name: "sgh_peft_glue" 
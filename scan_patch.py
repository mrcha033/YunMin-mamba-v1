# scan_patch.py
"""
ğŸ§  YunMin Correlation Scan íŒ¨ì¹˜ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì‚¬ì „ ê³„ì‚°ëœ ìˆœì—´(Ï€)ê³¼ ì—­ìˆœì—´(Ï€â»Â¹)ì„ Mamba ëª¨ë¸ì— ì ìš©í•˜ì—¬
Correlation Scan ê¸°ë²•ì„ ì‹¤í˜„í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    from scan_patch import apply_scan_patch, remove_scan_patch
    
    # ìˆœì—´ ì ìš©
    apply_scan_patch(model)
    
    # ìˆœì—´ ì œê±° (ì›ë³¸ ë³µì›)
    remove_scan_patch(model)
"""

import torch
import numpy as np
import os
from typing import Optional, Dict, Any

class ScanPatcher:
    def __init__(self):
        self.original_forwards = {}  # ì›ë³¸ forward í•¨ìˆ˜ ë°±ì—…
        self.is_patched = False
        self.pi = None
        self.pi_inv = None
        
    def load_permutations(self, scan_path="scan_order.npy", reverse_path="scan_order_inv.npy"):
        """ìˆœì—´ íŒŒì¼ë“¤ì„ ë¡œë”©"""
        if not os.path.exists(scan_path):
            raise FileNotFoundError(f"âŒ ìˆœì—´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scan_path}")
        if not os.path.exists(reverse_path):
            raise FileNotFoundError(f"âŒ ì—­ìˆœì—´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {reverse_path}")
            
        pi_np = np.load(scan_path)
        pi_inv_np = np.load(reverse_path)
        
        self.pi = torch.tensor(pi_np, dtype=torch.long)
        self.pi_inv = torch.tensor(pi_inv_np, dtype=torch.long)
        
        print(f"âœ… [SCAN PATCH] ìˆœì—´ ë¡œë”© ì™„ë£Œ:")
        print(f"   Ï€.shape: {self.pi.shape}")
        print(f"   Ï€â»Â¹.shape: {self.pi_inv.shape}")
        print(f"   ìˆœì—´ ë¯¸ë¦¬ë³´ê¸°: {self.pi[:10].tolist()}")
        
    def _create_patched_forward(self, original_forward, layer_idx):
        """íŒ¨ì¹˜ëœ forward í•¨ìˆ˜ ìƒì„±"""
        def patched_forward(hidden_states, *args, **kwargs):
            batch_size, seq_len, hidden_size = hidden_states.shape
            device = hidden_states.device
            
            # ë””ë°”ì´ìŠ¤ ì´ë™
            pi_device = self.pi.to(device)
            pi_inv_device = self.pi_inv.to(device)
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ ê²€ì¦ ë° ì¡°ì •
            perm_len = len(pi_device)
            
            if seq_len <= perm_len:
                # ì‹œí€€ìŠ¤ê°€ ìˆœì—´ë³´ë‹¤ ê°™ê±°ë‚˜ ì§§ì€ ê²½ìš°: ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
                valid_mask = pi_device < seq_len
                pi_filtered = pi_device[valid_mask]
                
                if len(pi_filtered) == 0:
                    # ìœ íš¨í•œ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìˆœì—´ ì ìš©í•˜ì§€ ì•ŠìŒ
                    hidden_states_permuted = hidden_states
                    pi_inv_for_output = torch.arange(seq_len, device=device)
                else:
                    # ìˆœì—´ ì ìš©
                    hidden_states_permuted = hidden_states[:, pi_filtered, :]
                    pi_inv_for_output = torch.argsort(pi_filtered)
                    
            else:
                # ì‹œí€€ìŠ¤ê°€ ìˆœì—´ë³´ë‹¤ ê¸´ ê²½ìš°: ìˆœì—´ì— ì—†ëŠ” ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ì¶”ê°€
                remaining_indices = torch.arange(perm_len, seq_len, device=device)
                pi_extended = torch.cat([pi_device, remaining_indices])
                pi_inv_for_output = torch.argsort(pi_extended)
                
                hidden_states_permuted = hidden_states[:, pi_extended, :]
            
            # ì›ë³¸ forward ì‹¤í–‰
            output = original_forward(hidden_states_permuted, *args, **kwargs)
            
            # ì¶œë ¥ ì—­ìˆœì—´ ì ìš© (í¬ê¸°ê°€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ)
            if output.shape[1] == hidden_states_permuted.shape[1] and len(pi_inv_for_output) == output.shape[1]:
                output_restored = output[:, pi_inv_for_output, :]
            else:
                # ì¶œë ¥ í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš° ë˜ëŠ” ì¸ë±ìŠ¤ ë¶ˆì¼ì¹˜ì‹œ ê·¸ëŒ€ë¡œ ë°˜í™˜
                output_restored = output
                
            return output_restored
            
        return patched_forward
    
    def apply_patch(self, model, target_layers="all"):
        """ëª¨ë¸ì— ìˆœì—´ íŒ¨ì¹˜ ì ìš©"""
        if self.pi is None or self.pi_inv is None:
            raise ValueError("âŒ ìˆœì—´ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_permutations()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            
        if self.is_patched:
            print("âš ï¸ ì´ë¯¸ íŒ¨ì¹˜ê°€ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € remove_patch()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.")
            return
            
        print(f"ğŸ”§ [SCAN PATCH] ìˆœì—´ íŒ¨ì¹˜ ì ìš© ì‹œì‘...")
        
        # Mamba ëª¨ë¸ êµ¬ì¡° í™•ì¸
        if hasattr(model, 'backbone'):
            layers = model.backbone.layers  # MambaForCausalLM
        elif hasattr(model, 'layers'):
            layers = model.layers  # MambaModel
        else:
            raise ValueError("âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ êµ¬ì¡°ì…ë‹ˆë‹¤.")
            
        patched_count = 0
        
        for i, layer in enumerate(layers):
            if target_layers != "all" and i not in target_layers:
                continue
                
            mixer = layer.mixer
            layer_id = f"layer_{i}_mixer"
            
            # ì›ë³¸ forward ë°±ì—…
            self.original_forwards[layer_id] = mixer.forward
            
            # íŒ¨ì¹˜ëœ forwardë¡œ êµì²´
            mixer.forward = self._create_patched_forward(mixer.forward, i)
            patched_count += 1
            
        self.is_patched = True
        print(f"âœ… [SCAN PATCH] íŒ¨ì¹˜ ì ìš© ì™„ë£Œ: {patched_count}ê°œ ë ˆì´ì–´")
        
    def remove_patch(self, model):
        """íŒ¨ì¹˜ ì œê±°í•˜ê³  ì›ë³¸ ë³µì›"""
        if not self.is_patched:
            print("âš ï¸ íŒ¨ì¹˜ê°€ ì ìš©ë˜ì§€ ì•Šì€ ìƒíƒœì…ë‹ˆë‹¤.")
            return
            
        print(f"ğŸ”„ [SCAN PATCH] íŒ¨ì¹˜ ì œê±° ì¤‘...")
        
        # Mamba ëª¨ë¸ êµ¬ì¡° í™•ì¸
        if hasattr(model, 'backbone'):
            layers = model.backbone.layers
        elif hasattr(model, 'layers'):
            layers = model.layers
        else:
            raise ValueError("âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ êµ¬ì¡°ì…ë‹ˆë‹¤.")
            
        restored_count = 0
        
        for i, layer in enumerate(layers):
            layer_id = f"layer_{i}_mixer"
            if layer_id in self.original_forwards:
                layer.mixer.forward = self.original_forwards[layer_id]
                restored_count += 1
                
        self.original_forwards.clear()
        self.is_patched = False
        print(f"âœ… [SCAN PATCH] íŒ¨ì¹˜ ì œê±° ì™„ë£Œ: {restored_count}ê°œ ë ˆì´ì–´ ë³µì›")

# ì „ì—­ íŒ¨ì²˜ ì¸ìŠ¤í„´ìŠ¤
_global_patcher = ScanPatcher()

def apply_scan_patch(model, scan_path="scan_order.npy", reverse_path="scan_order_inv.npy", target_layers="all"):
    """
    ëª¨ë¸ì— Correlation Scan íŒ¨ì¹˜ ì ìš©
    
    Args:
        model: Mamba ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        scan_path: ìˆœì—´ íŒŒì¼ ê²½ë¡œ
        reverse_path: ì—­ìˆœì—´ íŒŒì¼ ê²½ë¡œ
        target_layers: íŒ¨ì¹˜ë¥¼ ì ìš©í•  ë ˆì´ì–´ ("all" ë˜ëŠ” ë ˆì´ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸)
    """
    _global_patcher.load_permutations(scan_path, reverse_path)
    _global_patcher.apply_patch(model, target_layers)

def remove_scan_patch(model):
    """ëª¨ë¸ì—ì„œ Correlation Scan íŒ¨ì¹˜ ì œê±°"""
    _global_patcher.remove_patch(model)

def is_scan_patched():
    """í˜„ì¬ íŒ¨ì¹˜ ì ìš© ìƒíƒœ í™•ì¸"""
    return _global_patcher.is_patched

def get_permutation_info():
    """í˜„ì¬ ë¡œë”©ëœ ìˆœì—´ ì •ë³´ ë°˜í™˜"""
    if _global_patcher.pi is None:
        return None
    return {
        'pi_length': len(_global_patcher.pi),
        'pi_preview': _global_patcher.pi[:10].tolist() if len(_global_patcher.pi) >= 10 else _global_patcher.pi.tolist()
    } 
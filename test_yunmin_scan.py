# test_yunmin_scan.py
"""
ğŸ§ª YunMin Correlation Scan ì™„ì „ í…ŒìŠ¤íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì›ë³¸ Mamba ëª¨ë¸ ë¡œë”©
2. LoRA @ SSM-only ì ìš©
3. YunMin Scan Patch ì ìš©/ì œê±°
4. Before/After ì„±ëŠ¥ ë¹„êµ
5. í…ìŠ¤íŠ¸ ìƒì„± í’ˆì§ˆ ê²€ì¦
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType
import time
import numpy as np

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤
from scan_patch import apply_scan_patch, remove_scan_patch, is_scan_patched, get_permutation_info

def setup_model_and_tokenizer():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""
    print("ğŸš€ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...")
    
    model_id = "state-spaces/mamba-130m"
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float32,
        ignore_mismatched_sizes=True
    )
    tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")
    tokenizer.pad_token = tokenizer.eos_token
    
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return model, tokenizer

def setup_lora(model):
    """LoRA @ SSM-only ì„¤ì •"""
    print("ğŸ”§ LoRA @ SSM-only ì„¤ì • ì¤‘...")
    
    target_modules = [
        "mixer.in_proj",
        "mixer.x_proj", 
        "mixer.dt_proj",
        "mixer.out_proj"
    ]
    
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    lora_model = get_peft_model(model, lora_config)
    lora_model.print_trainable_parameters()
    
    return lora_model

def test_generation(model, tokenizer, prompt="ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”", max_tokens=30, label=""):
    """í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ“ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸{label}:")
    print(f"   í”„ë¡¬í”„íŠ¸: '{prompt}'")
    
    inputs = tokenizer(prompt, return_tensors="pt")
    
    start_time = time.time()
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    generation_time = time.time() - start_time
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"   ê²°ê³¼: '{generated_text}'")
    print(f"   ì†Œìš” ì‹œê°„: {generation_time:.3f}ì´ˆ")
    print()
    
    return generated_text, generation_time

def test_perplexity(model, tokenizer, text="The quick brown fox jumps over the lazy dog."):
    """ê°„ë‹¨í•œ perplexity í…ŒìŠ¤íŠ¸"""
    inputs = tokenizer(text, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs.input_ids)
        loss = outputs.loss.item()
        perplexity = torch.exp(torch.tensor(loss)).item()
    
    return perplexity

def main():
    print("=" * 70)
    print("ğŸ§  YunMin Correlation Scan ì™„ì „ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # 1. ëª¨ë¸ ì„¤ì •
    model, tokenizer = setup_model_and_tokenizer()
    
    # 2. LoRA ì ìš©
    lora_model = setup_lora(model)
    
    # 3. ìˆœì—´ íŒŒì¼ í™•ì¸
    import os
    if not os.path.exists("scan_order.npy"):
        print("âŒ scan_order.npyê°€ ì—†ìŠµë‹ˆë‹¤. run_correlation_scan.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    print("\n" + "=" * 50)
    print("ğŸ“Š BEFORE: ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •")
    print("=" * 50)
    
    # Before í…ŒìŠ¤íŠ¸
    test_prompts = [
        "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”",
        "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”", 
        "Today is a beautiful"
    ]
    
    before_results = {}
    for prompt in test_prompts:
        text, time_taken = test_generation(lora_model, tokenizer, prompt, label=" [BEFORE]")
        before_results[prompt] = {'text': text, 'time': time_taken}
    
    # Perplexity ì¸¡ì • (BEFORE)
    before_ppl = test_perplexity(lora_model, tokenizer)
    print(f"ğŸ“ˆ BEFORE Perplexity: {before_ppl:.4f}")
    
    print("\n" + "=" * 50)
    print("ğŸ”§ YunMin Correlation Scan íŒ¨ì¹˜ ì ìš©")
    print("=" * 50)
    
    # 4. Scan Patch ì ìš©
    try:
        apply_scan_patch(lora_model)
        
        # íŒ¨ì¹˜ ì •ë³´ í™•ì¸
        patch_info = get_permutation_info()
        print(f"ğŸ“‹ íŒ¨ì¹˜ ì •ë³´: {patch_info}")
        print(f"ğŸ”— íŒ¨ì¹˜ ìƒíƒœ: {is_scan_patched()}")
        
    except Exception as e:
        print(f"âŒ íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        return
    
    print("\n" + "=" * 50)
    print("ğŸ“Š AFTER: YunMin Scan ì ìš© í›„ ì„±ëŠ¥ ì¸¡ì •")
    print("=" * 50)
    
    # After í…ŒìŠ¤íŠ¸
    after_results = {}
    for prompt in test_prompts:
        text, time_taken = test_generation(lora_model, tokenizer, prompt, label=" [AFTER]")
        after_results[prompt] = {'text': text, 'time': time_taken}
    
    # Perplexity ì¸¡ì • (AFTER)
    after_ppl = test_perplexity(lora_model, tokenizer)
    print(f"ğŸ“ˆ AFTER Perplexity: {after_ppl:.4f}")
    
    print("\n" + "=" * 50)
    print("ğŸ“ˆ ê²°ê³¼ ë¹„êµ ë¶„ì„")
    print("=" * 50)
    
    # ê²°ê³¼ ë¹„êµ
    print("ğŸ” í…ìŠ¤íŠ¸ ìƒì„± ë¹„êµ:")
    for prompt in test_prompts:
        print(f"\ní”„ë¡¬í”„íŠ¸: '{prompt}'")
        print(f"  BEFORE: {before_results[prompt]['text']}")
        print(f"  AFTER:  {after_results[prompt]['text']}")
        
        time_diff = after_results[prompt]['time'] - before_results[prompt]['time']
        print(f"  ì‹œê°„ ë³€í™”: {time_diff:+.3f}ì´ˆ")
    
    print(f"\nğŸ“Š Perplexity ë³€í™”:")
    print(f"  BEFORE: {before_ppl:.4f}")
    print(f"  AFTER:  {after_ppl:.4f}")
    ppl_change = ((after_ppl - before_ppl) / before_ppl) * 100
    print(f"  ë³€í™”ìœ¨: {ppl_change:+.2f}%")
    
    # 6. íŒ¨ì¹˜ ì œê±° í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 50)
    print("ğŸ”„ íŒ¨ì¹˜ ì œê±° ë° ë³µì› í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    remove_scan_patch(lora_model)
    print(f"ğŸ”— íŒ¨ì¹˜ ì œê±° í›„ ìƒíƒœ: {is_scan_patched()}")
    
    # ë³µì› í™•ì¸
    restore_text, restore_time = test_generation(lora_model, tokenizer, test_prompts[0], label=" [RESTORED]")
    restore_ppl = test_perplexity(lora_model, tokenizer)
    
    print(f"ğŸ” ë³µì› ê²€ì¦:")
    print(f"  ì›ë³¸ê³¼ ë™ì¼í•œì§€: {abs(restore_ppl - before_ppl) < 0.001}")
    print(f"  ë³µì› Perplexity: {restore_ppl:.4f}")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ YunMin Correlation Scan í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)
    
    # ìµœì¢… ìš”ì•½
    print("ğŸ“‹ ìµœì¢… ìš”ì•½:")
    print(f"  ğŸ§  ìˆœì—´ ê¸¸ì´: {patch_info['pi_length'] if patch_info else 'N/A'}")
    print(f"  ğŸ“ˆ Perplexity ë³€í™”: {ppl_change:+.2f}%")
    print(f"  ğŸ”§ LoRA íŒŒë¼ë¯¸í„°: 1.48% (SSM-only)")
    print(f"  âœ… íŒ¨ì¹˜ ì ìš©/ì œê±°: ì •ìƒ ì‘ë™")

if __name__ == "__main__":
    main() 
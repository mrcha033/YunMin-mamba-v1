# extract_hidden_states.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset

def extract_hidden_states():
    print("ðŸš€ [Step 1] Hidden State Extraction ì‹œìž‘...")
    
    # ëª¨ë¸ ë¡œë”©
    model_id = "state-spaces/mamba-130m"
    print(f"ðŸ“¥ ëª¨ë¸ ë¡œë”©: {model_id}")
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        torch_dtype=torch.float32,
        ignore_mismatched_sizes=True
    )
    
    # MambaëŠ” GPTNeoX tokenizer ì‚¬ìš©
    tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")
    tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()

    # WikiText-2 ì†Œê·œëª¨ ìƒ˜í”Œ ë¡œë”©
    print("ðŸ“š WikiText-2 ë°ì´í„°ì…‹ ë¡œë”©...")
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1%]")

    hidden_states_list = []

    def capture_hook(module, input, output):
        """hidden statesë¥¼ ìº¡ì²˜í•˜ëŠ” í›… í•¨ìˆ˜"""
        # input[0]ì€ hidden_states (shape: [batch_size, seq_len, hidden_size])
        hidden_states_list.append(input[0].detach().cpu())

    # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ mixer ìž…ë ¥ì„ í›…ìœ¼ë¡œ ìº¡ì²˜
    # Mamba êµ¬ì¡°: model.backbone.layers[-1].mixer
    target_layer = model.backbone.layers[-1].mixer
    hook = target_layer.register_forward_hook(capture_hook)
    
    print("ðŸŽ¯ í›… ë“±ë¡ ì™„ë£Œ: ë§ˆì§€ë§‰ ë ˆì´ì–´ mixer ìž…ë ¥ ìº¡ì²˜")

    # ìµœëŒ€ 10ê°œ ë¬¸ìž¥ë§Œ ì²˜ë¦¬
    processed_count = 0
    for i, item in enumerate(dataset):
        if processed_count >= 10:
            break
            
        text = item["text"].strip()
        if len(text) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ìŠ¤í‚µ
            continue
            
        print(f"ðŸ”„ ì²˜ë¦¬ ì¤‘: {processed_count + 1}/10 - '{text[:50]}...'")
        
        try:
            inputs = tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=256,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¶•ì†Œ
                padding=True
            )
            
            with torch.no_grad():
                _ = model(**inputs)
                
            processed_count += 1
            
        except Exception as e:
            print(f"âš ï¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    hook.remove()
    print("ðŸ”Œ í›… ì œê±° ì™„ë£Œ")

    if not hidden_states_list:
        raise ValueError("âŒ Hidden statesê°€ ìº¡ì²˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

    # ëª¨ë“  hidden stateë¥¼ [ì´_í† í°ìˆ˜, hidden_size]ë¡œ ê²°í•©
    all_h_list = []
    for h in hidden_states_list:
        # h shape: [batch_size, seq_len, hidden_size]
        # -> [seq_len, hidden_size]
        all_h_list.append(h.squeeze(0))
    
    all_h = torch.cat(all_h_list, dim=0)  # [total_tokens, hidden_size]
    print(f"âœ… [INFO] Total hidden states: {all_h.shape}")

    # ì €ìž¥
    torch.save(all_h, "hidden_states.pt")
    print(f"ðŸ’¾ ì €ìž¥ ì™„ë£Œ: hidden_states.pt ({all_h.shape})")
    
    return all_h.shape

if __name__ == "__main__":
    extract_hidden_states() 
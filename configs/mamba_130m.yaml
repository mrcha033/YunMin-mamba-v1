# Mamba-130M Configuration
# Based on original Mamba paper specifications for 130M parameter model

model:
  # Core architecture parameters
  d_model: 768                    # Model dimension
  n_layer: 12                     # Number of transformer layers
  vocab_size: 50257               # GPT-2 vocabulary size
  
  # SSM-specific parameters
  d_state: 16                     # SSM state dimension
  d_conv: 4                       # 1D convolution kernel size
  expand: 2                       # Expansion factor for inner dimension
  dt_rank: "auto"                 # Delta rank (auto = ceil(d_model/16))
  
  # Additional parameters
  bias: false                     # Use bias in linear layers
  conv_bias: true                 # Use bias in conv layers
  pscan: true                     # Use parallel scan
  
  # Initialization
  initializer_range: 0.02         # Standard deviation for weight initialization
  rescale_prenorm_residual: true  # Rescale prenorm residual
  residual_in_fp32: true          # Keep residuals in fp32
  fused_add_norm: true            # Use fused add norm
  
  # Model size validation
  expected_params: 130_000_000    # Expected parameter count for validation

training:
  # Phase A: Pre-training hyperparameters
  pretrain:
    optimizer: "AdamW"
    learning_rate: 2e-4           # Peak learning rate for pre-training
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.98                   # Updated from 0.95 to 0.98
    eps: 1e-6                     # Updated from 1e-8 to 1e-6
    
    # Training schedule
    batch_size: 128               # Conceptual batch size
    micro_batch_size: 8           # Per-device batch size (adjust based on memory)
    gradient_accumulation_steps: 16  # 128 / 8 = 16 (will adjust dynamically if needed)
    max_epochs: 20
    warmup_steps_ratio: 0.1       # Linear warmup for first 10% of training steps
    
    # Regularization
    max_grad_norm: 1.0
    dropout: 0.0
    
    # Learning rate schedule - Cosine annealing with linear warmup
    lr_scheduler: "cosine"
    min_lr_ratio: 0.1             # min_lr = 0.1 * learning_rate
    
  # Phase B: Fine-tuning hyperparameters  
  finetune:
    optimizer: "AdamW"
    learning_rate: 1e-4           # Peak learning rate for fine-tuning
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.98                   # Consistent with pre-training
    eps: 1e-6                     # Consistent with pre-training
    
    # Task-specific settings
    batch_size: 32                # Conceptual batch size for fine-tuning
    micro_batch_size: 8           # Per-device batch size
    gradient_accumulation_steps: 4  # 32 / 8 = 4
    
    # Learning rate schedule
    lr_scheduler: "cosine"
    warmup_steps_ratio: 0.1       # 10% linear warmup
    min_lr_ratio: 0.1
    
    # Task-specific epochs (per experimental description)
    # Default subset: sst2, mrpc, qnli, mnli
    epochs:
      sst2: 5   # Match experimental description
      mnli: 10  # Match experimental description (was 8)
      qnli: 5   # Standard for this task
      mrpc: 8   # Standard for this task
      cola: 10
      stsb: 6
      qqp: 3
      rte: 10
      wnli: 10
    
    # Early stopping to prevent overfitting
    early_stopping_patience: 3
    early_stopping_threshold: 1e-4
    monitor_metric: "eval_accuracy"  # Primary metric for early stopping

# SDM-specific configuration
sdm:
  lambda_sparsity: 0.01           # Sparsity regularization weight
  gumbel_temp_start: 5.0          # Initial Gumbel temperature
  gumbel_temp_end: 0.1            # Final Gumbel temperature
  temp_anneal_steps_ratio: 0.8    # Anneal over 80% of training
  target_sparsity: 0.3            # Target 30% sparsity
  
# SGH-PEFT configuration
sgh_peft:
  # LoRA parameters
  lora_high_rank: 16
  lora_low_rank: 4
  lora_alpha_factor: 2
  lora_dropout: 0.05
  
  # Importance thresholds (θ for SGH-PEFT as 75th percentile)
  # Will be explored as hyperparameter with sensitivity analysis in appendix
  importance_percentile: 75        # 75th percentile threshold for layer importance
  high_importance_mean_threshold: 0.5
  high_importance_active_threshold: 60.0
  medium_importance_mean_threshold: 0.0
  medium_importance_active_threshold: 40.0
  low_importance_mean_threshold: -0.5
  
  # Adaptation settings
  apply_sparsity_mask: true
  freeze_base_model: true
  
  # Hyperparameter exploration
  sensitivity_analysis: true       # Include sensitivity analysis for threshold θ

# Hardware and system configuration
system:
  # Target hardware - NVIDIA A100 (80GB memory)
  device: "cuda"
  mixed_precision: "bf16"         # PyTorch AMP with bfloat16 dtype for A100
  compile: true                   # torch.compile for optimization
  
  # CUDA environment
  cuda_version: "12.1"
  pytorch_version: "2.2"
  
  # Memory optimization for A100
  gradient_checkpointing: true
  dataloader_num_workers: 4
  pin_memory: true
  
  # Distributed training
  ddp: true                       # DistributedDataParallel
  find_unused_parameters: false
  
  # Reproducibility settings
  deterministic: true             # torch.backends.cudnn.deterministic = True
  benchmark: false                # torch.backends.cudnn.benchmark = False (for reproducibility)
  seed: 42                        # Fixed random seed across all libraries
  
# Evaluation configuration
evaluation:
  # Validation frequency
  eval_interval: 1000             # Evaluate every 1000 steps
  save_interval: 5000             # Save checkpoint every 5000 steps
  
  # Profiling Tools (fvcore for FLOPs, torch.profiler for latency/memory)
  compute_flops: true             # fvcore for FLOPs calculation
  profile_memory: true            # torch.profiler for memory profiling
  measure_latency: true           # torch.profiler for detailed latency profiling
  archive_profiling_logs: true    # Archive profiling logs with unique experiment IDs
  
  # Statistical testing for reproducibility
  num_seeds: 5                    # Run with 5 different seeds (seed=42 base)
  confidence_level: 0.95          # 95% confidence intervals

# Logging and checkpointing
logging:
  project_name: "mamba-130m-codesign"
  experiment_name: "full-scale-validation"
  log_interval: 100
  
  # Weights & Biases integration
  use_wandb: true
  wandb_project: "hardware-data-parameter-codesign"
  
  # Checkpoint management
  save_top_k: 3                   # Keep top 3 checkpoints
  monitor_metric: "eval/perplexity"
  mode: "min"

# Reproducibility (Scientific Rigor)
seed: 42                          # Fixed random seed for PyTorch, NumPy, Python
deterministic: true               # torch.backends.cudnn.deterministic = True
benchmark: false                  # torch.backends.cudnn.benchmark = False (for reproducibility)

# Note: Acknowledging potential minor trade-off in performance for scientific rigor 